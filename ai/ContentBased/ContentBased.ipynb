{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587a81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ac016",
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles = pd.read_csv('../instacart/aisles.csv')\n",
    "departments = pd.read_csv('../instacart/departments.csv')\n",
    "# orderproducts_prior = pd.read_csv('../instacart/order_products__prior.csv')\n",
    "orderproducts = pd.read_csv('../instacart/order_products__train.csv')\n",
    "orders = pd.read_csv('../instacart/orders.csv')\n",
    "products = pd.read_csv('../instacart/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28e6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA & Data Preprocessing\n",
    "# orderproducts = pd.concat([orderproducts_prior, orderproducts_tests], axis=0)\n",
    "orders = orders[orders['eval_set'] != 'prior']\n",
    "userproducts = orderproducts.merge(orders, on='order_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f28931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reordered</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112108</td>\n",
       "      <td>49302</td>\n",
       "      <td>1</td>\n",
       "      <td>Bulgarian Yogurt</td>\n",
       "      <td>yogurt</td>\n",
       "      <td>dairy eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112108</td>\n",
       "      <td>11109</td>\n",
       "      <td>1</td>\n",
       "      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n",
       "      <td>other creams cheeses</td>\n",
       "      <td>dairy eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112108</td>\n",
       "      <td>10246</td>\n",
       "      <td>0</td>\n",
       "      <td>Organic Celery Hearts</td>\n",
       "      <td>fresh vegetables</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112108</td>\n",
       "      <td>49683</td>\n",
       "      <td>0</td>\n",
       "      <td>Cucumber Kirby</td>\n",
       "      <td>fresh vegetables</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112108</td>\n",
       "      <td>43633</td>\n",
       "      <td>1</td>\n",
       "      <td>Lightly Smoked Sardines in Olive Oil</td>\n",
       "      <td>canned meat seafood</td>\n",
       "      <td>canned goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384612</th>\n",
       "      <td>169679</td>\n",
       "      <td>14233</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Artesian Water</td>\n",
       "      <td>water seltzer sparkling water</td>\n",
       "      <td>beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384613</th>\n",
       "      <td>169679</td>\n",
       "      <td>35548</td>\n",
       "      <td>1</td>\n",
       "      <td>Twice Baked Potatoes</td>\n",
       "      <td>prepared meals</td>\n",
       "      <td>deli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384614</th>\n",
       "      <td>139822</td>\n",
       "      <td>35951</td>\n",
       "      <td>1</td>\n",
       "      <td>Organic Unsweetened Almond Milk</td>\n",
       "      <td>soy lactosefree</td>\n",
       "      <td>dairy eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384615</th>\n",
       "      <td>139822</td>\n",
       "      <td>16953</td>\n",
       "      <td>1</td>\n",
       "      <td>Creamy Peanut Butter</td>\n",
       "      <td>spreads</td>\n",
       "      <td>pantry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384616</th>\n",
       "      <td>139822</td>\n",
       "      <td>4724</td>\n",
       "      <td>1</td>\n",
       "      <td>Broccoli Florettes</td>\n",
       "      <td>packaged produce</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1384617 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  product_id  reordered  \\\n",
       "0         112108       49302          1   \n",
       "1         112108       11109          1   \n",
       "2         112108       10246          0   \n",
       "3         112108       49683          0   \n",
       "4         112108       43633          1   \n",
       "...          ...         ...        ...   \n",
       "1384612   169679       14233          1   \n",
       "1384613   169679       35548          1   \n",
       "1384614   139822       35951          1   \n",
       "1384615   139822       16953          1   \n",
       "1384616   139822        4724          1   \n",
       "\n",
       "                                          product_name  \\\n",
       "0                                     Bulgarian Yogurt   \n",
       "1        Organic 4% Milk Fat Whole Milk Cottage Cheese   \n",
       "2                                Organic Celery Hearts   \n",
       "3                                       Cucumber Kirby   \n",
       "4                 Lightly Smoked Sardines in Olive Oil   \n",
       "...                                                ...   \n",
       "1384612                         Natural Artesian Water   \n",
       "1384613                           Twice Baked Potatoes   \n",
       "1384614                Organic Unsweetened Almond Milk   \n",
       "1384615                           Creamy Peanut Butter   \n",
       "1384616                             Broccoli Florettes   \n",
       "\n",
       "                                 aisle    department  \n",
       "0                               yogurt    dairy eggs  \n",
       "1                 other creams cheeses    dairy eggs  \n",
       "2                     fresh vegetables       produce  \n",
       "3                     fresh vegetables       produce  \n",
       "4                  canned meat seafood  canned goods  \n",
       "...                                ...           ...  \n",
       "1384612  water seltzer sparkling water     beverages  \n",
       "1384613                 prepared meals          deli  \n",
       "1384614                soy lactosefree    dairy eggs  \n",
       "1384615                        spreads        pantry  \n",
       "1384616               packaged produce       produce  \n",
       "\n",
       "[1384617 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_userproducts = userproducts[['user_id','product_id','reordered']]\n",
    "detailed_userproducts = detailed_userproducts.merge(products, on='product_id', how='inner')\n",
    "detailed_userproducts = detailed_userproducts.merge(aisles, on='aisle_id', how='inner')\n",
    "detailed_userproducts = detailed_userproducts.merge(departments, on='department_id', how='inner')\n",
    "detailed_userproducts = detailed_userproducts.drop(columns=['aisle_id','department_id'])\n",
    "detailed_userproducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d318c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    punctuation = string.punctuation\n",
    "    stopwordlist = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in stopwordlist]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# detailed_userproducts['product_name'] = detailed_userproducts['product_name'].apply(lambda x: text_cleaning(x))\n",
    "# detailed_userproducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe7c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(text_columns, model_path='./tfidf_vectorizer.pkl'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing TFIDF model from {model_path}\")\n",
    "        with open(model_path, 'rb') as f:\n",
    "            tfidf_vectorizer = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Training a new TFIDF Vectorizer and saving it to {model_path}\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "        tfidf_vectorizer.fit(text_columns)\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tfidf_vectorizer, f)\n",
    "    tfidf = tfidf_vectorizer.transform(text_columns)\n",
    "    tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf, index=text_columns.index)\n",
    "    return tfidf, tfidf_df\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "# train_tfidf = tfidf_vectorizer.fit_transform(xtrain['product_name'])\n",
    "# test_tfidf = tfidf_vectorizer.transform(xtest['product_name'])\n",
    "# train_tfidf = tfidf(xtrain['product_name'])\n",
    "# test_tfidf = tfidf(xtest['product_name'])\n",
    "\n",
    "def bow(text_columns, model_path='./bow_vectorizer.pkl'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing bow model from {model_path}\")\n",
    "        with open(model_path, 'rb') as f:\n",
    "            bow_vectorizer = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Training a new bow Vectorizer and saving it to {model_path}\")\n",
    "        bow_vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "        bow_vectorizer.fit(text_columns)\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(bow_vectorizer, f)\n",
    "    bow = bow_vectorizer.transform(text_columns)\n",
    "    bow_df = pd.DataFrame.sparse.from_spmatrix(bow, index=text_columns.index)\n",
    "    return bow, bow_df\n",
    "\n",
    "# bow_vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "# train_bow = bow_vectorizer.fit_transform(xtrain['product_name'])\n",
    "# test_bow = bow_vectorizer.transform(xtest['product_name'])\n",
    "# train_bow = bow(xtrain['product_name'])\n",
    "# test_bow = bow(xtest['product_name'])\n",
    "\n",
    "def get_product_vector(product_name, model):\n",
    "    tokens = word_tokenize(product_name)\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vectors) == 0: return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def cbow(text_columns, model_path='./word2vec_cbow.model'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing Word2Vec CBOW model from {model_path}\")\n",
    "        semantic_cbow = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        print(f\"Training a new Word2Vec CBOW model and saving it to {model_path}\")\n",
    "        tokenized = text_columns.apply(word_tokenize)\n",
    "        semantic_cbow = Word2Vec(sentences=tokenized, min_count=1, vector_size=100, window=5, sg=0)\n",
    "        semantic_cbow.save(model_path)\n",
    "    cbow_vectors = text_columns.apply(lambda x: get_product_vector(x, semantic_cbow))\n",
    "    cbow_vectors_df = pd.DataFrame(cbow_vectors.tolist(), index=text_columns.index)\n",
    "    return cbow_vectors, cbow_vectors_df\n",
    "\n",
    "# train_cbow = cbow(xtrain['product_name'])\n",
    "# test_cbow = cbow(xtest['product_name'])\n",
    "\n",
    "def skipgram(text_columns, model_path='./word2vec_skipgram.model'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing Word2Vec Skipgram model from {model_path}\")\n",
    "        semantic_skipgram = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        print(f\"Training a new Word2Vec Skipgram model and saving it to {model_path}\")\n",
    "        tokenized = text_columns.apply(word_tokenize)\n",
    "        semantic_skipgram = Word2Vec(sentences=tokenized, min_count=1, vector_size=100, window=5, sg=1)\n",
    "        semantic_skipgram.save(model_path)\n",
    "    skipgram_vectors = text_columns.apply(lambda x: get_product_vector(x, semantic_skipgram))\n",
    "    skipgram_vectors_df = pd.DataFrame(skipgram_vectors.tolist(), index=text_columns.index)\n",
    "    return skipgram_vectors, skipgram_vectors_df\n",
    "\n",
    "# train_skipgram = skipgram(xtrain['product_name'])\n",
    "# test_skipgram = skipgram(xtest['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b1f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_bow_df = pd.DataFrame.sparse.from_spmatrix(train_bow, index=xtrain.index)\n",
    "# test_bow_df = pd.DataFrame.sparse.from_spmatrix(test_bow, index=xtest.index)\n",
    "# _, train_bow_df = bow(xtrain['product_name'])\n",
    "# _, test_bow_df = bow(xtest['product_name'])\n",
    "\n",
    "# train_tfidf_df = pd.DataFrame.sparse.from_spmatrix(train_tfidf, index=xtrain.index)\n",
    "# test_tfidf_df = pd.DataFrame.sparse.from_spmatrix(test_tfidf, index=xtest.index)\n",
    "# _, train_tfidf_df = tfidf(xtrain['product_name'])\n",
    "# _, test_tfidf_df = tfidf(xtest['product_name'])\n",
    "\n",
    "# train_cbow_df = pd.DataFrame(train_cbow.tolist(), index=xtrain.index)\n",
    "# test_cbow_df = pd.DataFrame(test_cbow.tolist(), index=xtest.index)\n",
    "# _, train_cbow_df = cbow(xtrain['product_name'])\n",
    "# _, test_cbow_df = cbow(xtest['product_name'])\n",
    "\n",
    "# train_skipgram_df = pd.DataFrame(train_skipgram.tolist(), index=xtrain.index)\n",
    "# test_skipgram_df = pd.DataFrame(test_skipgram.tolist(), index=xtest.index)\n",
    "# _, train_skipgram_df = skipgram(xtrain['product_name'])\n",
    "# _, test_skipgram_df = skipgram(xtest['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3350dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_aisle(dataset, model_path='./aisle_onehotencoder.pkl'):\n",
    "#     if os.path.exists(model_path):\n",
    "#         print(f\"Loading existing encoder from {model_path}\")\n",
    "#         encoder = pd.read_pickle(model_path)\n",
    "#     else:\n",
    "#         print(f\"Training a new aisle encoder and saving it to {model_path}\")\n",
    "#         encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "#         encoder.fit(dataset[['aisle']])\n",
    "#         pd.to_pickle(encoder, model_path)\n",
    "#     dataset_encoded = encoder.transform(dataset[['aisle']])\n",
    "#     dataset_encoded_df = pd.DataFrame(dataset_encoded.toarray(), columns=encoder.get_feature_names_out(['aisle']), index=dataset.index)\n",
    "#     return dataset_encoded, dataset_encoded_df\n",
    "\n",
    "# def encode_department(dataset, model_path='./department_onehotencoder.pkl'):\n",
    "#     if os.path.exists(model_path):\n",
    "#         print(f\"Loading existing encoder from {model_path}\")\n",
    "#         encoder = pd.read_pickle(model_path)\n",
    "#     else:\n",
    "#         print(f\"Training a new department encoder and saving it to {model_path}\")\n",
    "#         encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "#         encoder.fit(dataset[['department']])\n",
    "#         pd.to_pickle(encoder, model_path)\n",
    "#     dataset_encoded = encoder.transform(dataset[['department']])\n",
    "#     dataset_encoded_df = pd.DataFrame(dataset_encoded.toarray(), columns=encoder.get_feature_names_out(['department']), index=dataset.index)\n",
    "#     return dataset_encoded, dataset_encoded_df\n",
    "\n",
    "def encode_aisle(dataset, model_path='./aisle_labelencoder.pkl'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing encoder from {model_path}\")\n",
    "        encoder = pd.read_pickle(model_path)\n",
    "    else:\n",
    "        print(f\"Training a new aisle encoder and saving it to {model_path}\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(dataset['aisle'])\n",
    "        pd.to_pickle(encoder, model_path)\n",
    "    dataset_encoded = encoder.transform(dataset['aisle'])\n",
    "    dataset_encoded_df = pd.DataFrame(dataset_encoded, columns=['aisle'], index=dataset.index)\n",
    "    return dataset_encoded, dataset_encoded_df\n",
    "\n",
    "def encode_department(dataset, model_path='./department_labelencoder.pkl'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing encoder from {model_path}\")\n",
    "        encoder = pd.read_pickle(model_path)\n",
    "    else:\n",
    "        print(f\"Training a new department encoder and saving it to {model_path}\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(dataset['department'])\n",
    "        pd.to_pickle(encoder, model_path)\n",
    "    dataset_encoded = encoder.transform(dataset['department'])\n",
    "    dataset_encoded_df = pd.DataFrame(dataset_encoded, columns=['department'], index=dataset.index)\n",
    "    return dataset_encoded, dataset_encoded_df\n",
    "\n",
    "# encoder_aisle = OneHotEncoder(handle_unknown='ignore')\n",
    "# encoder_department = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# xtrain_aisle_encoded = encoder_aisle.fit_transform(xtrain[['aisle']])\n",
    "# xtest_aisle_encoded = encoder_aisle.transform(xtest[['aisle']])\n",
    "# xtrain_department_encoded = encoder_department.fit_transform(xtrain[['department']])\n",
    "# xtest_department_encoded = encoder_department.transform(xtest[['department']])\n",
    "\n",
    "# xtrain_aisle_encoded = pd.DataFrame(xtrain_aisle_encoded.toarray(), columns=encoder_aisle.get_feature_names_out(['aisle']), index=xtrain.index)\n",
    "# xtest_aisle_encoded = pd.DataFrame(xtest_aisle_encoded.toarray(), columns=encoder_aisle.get_feature_names_out(['aisle']), index=xtest.index)\n",
    "# xtrain_department_encoded = pd.DataFrame(xtrain_department_encoded.toarray(), columns=encoder_department.get_feature_names_out(['department']), index=xtrain.index)\n",
    "# xtest_department_encoded = pd.DataFrame(xtest_department_encoded.toarray(), columns=encoder_department.get_feature_names_out(['department']), index=xtest.index)\n",
    "\n",
    "# xtrain = xtrain.drop(columns=['aisle', 'department'])\n",
    "# xtest = xtest.drop(columns=['aisle', 'department'])\n",
    "\n",
    "# xtrain = pd.concat([xtrain, xtrain_aisle_encoded, xtrain_department_encoded], axis=1)\n",
    "# xtest = pd.concat([xtest, xtest_aisle_encoded, xtest_department_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962a4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(dataset):\n",
    "    print('Cleaning & preprocess text for feature extraction... (1/6)')\n",
    "    dataset['product_name'] = dataset['product_name'].apply(lambda x: text_cleaning(x))\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    print('Product name feature extraction... (2/6)')\n",
    "    _, tfidf_vector = tfidf(dataset['product_name'])\n",
    "    tfidf_vector = tfidf_vector.reset_index(drop=True)\n",
    "\n",
    "    print('Aisle name one-hot encoding... (3/6)')\n",
    "    _, aisle_encoded = encode_aisle(dataset)\n",
    "    aisle_encoded = aisle_encoded.reset_index(drop=True)\n",
    "\n",
    "    print('Department name one-hot encoding... (4/6)')\n",
    "    _, department_encoded = encode_department(dataset)\n",
    "    department_encoded = department_encoded.reset_index(drop=True)\n",
    "\n",
    "    print('Combining all features csr sparse matrix... (5/6)')\n",
    "    aisle_encoded_sparse = csr_matrix(aisle_encoded.values)\n",
    "    department_encoded_sparse = csr_matrix(department_encoded.values)\n",
    "    features_only = hstack([tfidf_vector, aisle_encoded_sparse, department_encoded_sparse])\n",
    "    features_only = features_only.tocsr()\n",
    "    userproduct_indexes = dataset[['user_id','product_id','reordered']]\n",
    "\n",
    "    return features_only, userproduct_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a294544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(detailed_userproducts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05faf275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning & preprocess text for feature extraction... (1/6)\n",
      "Product name feature extraction... (2/6)\n",
      "Loading existing TFIDF model from ./tfidf_vectorizer.pkl\n",
      "Aisle name one-hot encoding... (3/6)\n",
      "Training a new aisle encoder and saving it to ./aisle_labelencoder.pkl\n",
      "Department name one-hot encoding... (4/6)\n",
      "Training a new department encoder and saving it to ./department_labelencoder.pkl\n",
      "Combining all features csr sparse matrix... (5/6)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 62.7 GiB for an array with shape (7599, 1107693) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features, train_userproduct_indexes \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# train_combined.columns = train_combined.columns.astype(str)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train_features.columns = train_features.columns.astype(str)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# train_userproduct_indexes.columns = train_userproduct_indexes.columns.astype(str)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m train_features\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mextract_feature\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     19\u001b[0m aisle_encoded_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(aisle_encoded\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     20\u001b[0m department_encoded_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(department_encoded\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m---> 21\u001b[0m features_only \u001b[38;5;241m=\u001b[39m \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtfidf_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maisle_encoded_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepartment_encoded_sparse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m features_only \u001b[38;5;241m=\u001b[39m features_only\u001b[38;5;241m.\u001b[39mtocsr()\n\u001b[0;32m     23\u001b[0m userproduct_indexes \u001b[38;5;241m=\u001b[39m dataset[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreordered\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\scipy\\sparse\\_construct.py:752\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhstack\u001b[39m(blocks, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m    Stack sparse matrices horizontally (column wise)\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    750\u001b[0m \n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 752\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(b, sparray) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39mflat):\n\u001b[0;32m    754\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _block([blocks], \u001b[38;5;28mformat\u001b[39m, dtype)\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\pandas\\core\\generic.py:2152\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m   2150\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2152\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\n\u001b[0;32m   2153\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2155\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2158\u001b[0m     ):\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\pandas\\core\\frame.py:1127\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[0;32m   1129\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\pandas\\core\\frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12594\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32mc:\\Users\\hi\\miniconda3\\envs\\ai_learning\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 62.7 GiB for an array with shape (7599, 1107693) and data type float64"
     ]
    }
   ],
   "source": [
    "train_features, train_userproduct_indexes = extract_feature(trainset)\n",
    "\n",
    "# train_combined.columns = train_combined.columns.astype(str)\n",
    "# train_features.columns = train_features.columns.astype(str)\n",
    "# train_userproduct_indexes.columns = train_userproduct_indexes.columns.astype(str)\n",
    "\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d613900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning & preprocess text for feature extraction... (1/6)\n",
      "Product name feature extraction... (2/6)\n",
      "Loading existing TFIDF model from ./tfidf_vectorizer.pkl\n",
      "Aisle name one-hot encoding... (3/6)\n",
      "Loading existing encoder from ./aisle_encoder.pkl\n",
      "Department name one-hot encoding... (4/6)\n",
      "Loading existing encoder from ./department_encoder.pkl\n",
      "Removing raw feature columns... (5/6)\n",
      "Reset indexes and setup for model training... (6/6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reordered</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>department_household</th>\n",
       "      <th>department_international</th>\n",
       "      <th>department_meat seafood</th>\n",
       "      <th>department_missing</th>\n",
       "      <th>department_other</th>\n",
       "      <th>department_pantry</th>\n",
       "      <th>department_personal care</th>\n",
       "      <th>department_pets</th>\n",
       "      <th>department_produce</th>\n",
       "      <th>department_snacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149174</td>\n",
       "      <td>37203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11333</td>\n",
       "      <td>35108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31224</td>\n",
       "      <td>37141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137296</td>\n",
       "      <td>12834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133811</td>\n",
       "      <td>48775</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276919</th>\n",
       "      <td>205958</td>\n",
       "      <td>19219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276920</th>\n",
       "      <td>31185</td>\n",
       "      <td>4920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276921</th>\n",
       "      <td>189092</td>\n",
       "      <td>22035</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276922</th>\n",
       "      <td>42945</td>\n",
       "      <td>12614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276923</th>\n",
       "      <td>176351</td>\n",
       "      <td>25949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276924 rows Ã— 7757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  product_id  reordered  0  1  2  3  4  5  6  ...  \\\n",
       "0        149174       37203          0  0  0  0  0  0  0  0  ...   \n",
       "1         11333       35108          1  0  0  0  0  0  0  0  ...   \n",
       "2         31224       37141          0  0  0  0  0  0  0  0  ...   \n",
       "3        137296       12834          0  0  0  0  0  0  0  0  ...   \n",
       "4        133811       48775          0  0  0  0  0  0  0  0  ...   \n",
       "...         ...         ...        ... .. .. .. .. .. .. ..  ...   \n",
       "276919   205958       19219          1  0  0  0  0  0  0  0  ...   \n",
       "276920    31185        4920          0  0  0  0  0  0  0  0  ...   \n",
       "276921   189092       22035          1  0  0  0  0  0  0  0  ...   \n",
       "276922    42945       12614          1  0  0  0  0  0  0  0  ...   \n",
       "276923   176351       25949          1  0  0  0  0  0  0  0  ...   \n",
       "\n",
       "        department_household  department_international  \\\n",
       "0                        1.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        0.0                       0.0   \n",
       "4                        0.0                       0.0   \n",
       "...                      ...                       ...   \n",
       "276919                   1.0                       0.0   \n",
       "276920                   0.0                       0.0   \n",
       "276921                   0.0                       0.0   \n",
       "276922                   0.0                       0.0   \n",
       "276923                   0.0                       0.0   \n",
       "\n",
       "        department_meat seafood  department_missing  department_other  \\\n",
       "0                           0.0                 0.0               0.0   \n",
       "1                           0.0                 0.0               0.0   \n",
       "2                           0.0                 0.0               0.0   \n",
       "3                           0.0                 0.0               0.0   \n",
       "4                           0.0                 0.0               0.0   \n",
       "...                         ...                 ...               ...   \n",
       "276919                      0.0                 0.0               0.0   \n",
       "276920                      0.0                 0.0               0.0   \n",
       "276921                      0.0                 0.0               0.0   \n",
       "276922                      0.0                 0.0               0.0   \n",
       "276923                      0.0                 0.0               0.0   \n",
       "\n",
       "        department_pantry  department_personal care  department_pets  \\\n",
       "0                     0.0                       0.0              0.0   \n",
       "1                     0.0                       0.0              0.0   \n",
       "2                     0.0                       0.0              0.0   \n",
       "3                     1.0                       0.0              0.0   \n",
       "4                     0.0                       0.0              0.0   \n",
       "...                   ...                       ...              ...   \n",
       "276919                0.0                       0.0              0.0   \n",
       "276920                0.0                       0.0              0.0   \n",
       "276921                0.0                       0.0              0.0   \n",
       "276922                0.0                       0.0              0.0   \n",
       "276923                0.0                       0.0              0.0   \n",
       "\n",
       "        department_produce  department_snacks  \n",
       "0                      0.0                0.0  \n",
       "1                      0.0                0.0  \n",
       "2                      0.0                0.0  \n",
       "3                      0.0                0.0  \n",
       "4                      1.0                0.0  \n",
       "...                    ...                ...  \n",
       "276919                 0.0                0.0  \n",
       "276920                 1.0                0.0  \n",
       "276921                 0.0                0.0  \n",
       "276922                 1.0                0.0  \n",
       "276923                 0.0                0.0  \n",
       "\n",
       "[276924 rows x 7757 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features, test_userproduct_indexes = extract_feature(testset)\n",
    "\n",
    "# test_combined.columns = test_combined.columns.astype(str)\n",
    "# test_features.columns = test_features.columns.astype(str)\n",
    "# test_userproduct_indexes.columns = test_userproduct_indexes.columns.astype(str)\n",
    "\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd11983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Train the model on the training set using product features\n",
    "# train_combined.columns = train_combined.columns.astype(str)\n",
    "# train_features.columns = train_features.columns.astype(str)\n",
    "# train_userproduct_indexes.columns = train_userproduct_indexes.columns.astype(str)\n",
    "# test_combined.columns = test_combined.columns.astype(str)\n",
    "# test_features.columns = test_features.columns.astype(str)\n",
    "# test_userproduct_indexes.columns = test_userproduct_indexes.columns.astype(str)\n",
    "\n",
    "# train_features = csr_matrix(train_features)\n",
    "# test_features = csr_matrix(test_features)\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=5, metric='cosine', n_jobs=-1)\n",
    "model.fit(train_features)\n",
    "\n",
    "# Step 4: Make recommendations on the test set (or for a specific user)\n",
    "user = testset['user_id'].unique()[0]  # Example: First user in the test set\n",
    "userpurchases = testset[testset['user_id'] == user]['product_id']\n",
    "\n",
    "recommendations = {}\n",
    "for product in userpurchases:\n",
    "    # Extract feature vector for the product from the test set\n",
    "    product_vector = test_features.loc[test_features['product_id'] == product].values.flatten()\n",
    "    \n",
    "    # Get similar products\n",
    "    similarity, indices = model.kneighbors([product_vector])\n",
    "    \n",
    "    # Map the indices back to product IDs\n",
    "    recommended_product_ids = test_combined.iloc[indices[0]]['product_id'].values\n",
    "    recommendations[product] = recommended_product_ids\n",
    "\n",
    "# Display recommendations for the test user\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd98faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reordered                                int64\n",
       "0                           Sparse[float64, 0]\n",
       "1                           Sparse[float64, 0]\n",
       "2                           Sparse[float64, 0]\n",
       "3                           Sparse[float64, 0]\n",
       "                                   ...        \n",
       "department_pantry                      float64\n",
       "department_personal care               float64\n",
       "department_pets                        float64\n",
       "department_produce                     float64\n",
       "department_snacks                      float64\n",
       "Length: 7755, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
